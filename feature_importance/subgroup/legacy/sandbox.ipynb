{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imodels imports\n",
    "from imodels import get_clean_dataset\n",
    "from imodels.tree.rf_plus.rf_plus.rf_plus_models import RandomForestPlusClassifier, RandomForestPlusRegressor\n",
    "from imodels.tree.rf_plus.feature_importance.rfplus_explainer import  AloRFPlusMDI, RFPlusMDI\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegressionCV, LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# other important libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get abalone data\n",
    "X, y, feature_names = get_clean_dataset(\"compas_two_year_clean\", data_source='imodels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    8.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF+ Test Set Accuracy: 0.7046436285097192\n",
      "RF+ Baseline Test Set Accuracy: 0.6868250539956804\n"
     ]
    }
   ],
   "source": [
    "# fit rf\n",
    "rf = RandomForestClassifier(n_estimators=100, max_features='sqrt',\n",
    "                            min_samples_leaf=5, random_state=1)\n",
    "rf_baseline = RandomForestRegressor(n_estimators=100, max_features='sqrt',\n",
    "                                    min_samples_leaf=5, random_state=1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "\n",
    "# fit rf+\n",
    "rf_plus = RandomForestPlusClassifier(rf_model = rf,\n",
    "                                     prediction_model = LogisticRegressionCV())\n",
    "rf_plus_baseline = RandomForestPlusRegressor(rf_model = rf_baseline,\n",
    "                                             include_raw=False, fit_on='inbag',\n",
    "                                             prediction_model = LinearRegression())\n",
    "rf_plus.fit(X_train, y_train)\n",
    "rf_plus_baseline.fit(X_train, y_train)\n",
    "\n",
    "# check performance on test set\n",
    "yhat_rfplus = rf_plus.predict(X_test)\n",
    "yhat_rfplus_baseline = rf_plus_baseline.predict(X_test)\n",
    "\n",
    "# evaluate accuracy on test set\n",
    "accuracy_rf_plus = accuracy_score(y_test, yhat_rfplus)\n",
    "accuracy_rf_plus_baseline = accuracy_score(y_test, yhat_rfplus_baseline > 0.5)\n",
    "print(f'RF+ Test Set Accuracy: {accuracy_rf_plus}')\n",
    "print(f'RF+ Baseline Test Set Accuracy: {accuracy_rf_plus_baseline}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create explainers\n",
    "mdi_explainer = RFPlusMDI(rf_plus)\n",
    "baseline_explainer = RFPlusMDI(rf_plus_baseline, evaluate_on='inbag')\n",
    "\n",
    "# get feature importances for train and test sets\n",
    "mdi_train_values = mdi_explainer.explain_linear_partial(X_train, y_train, l2norm = True, sign = True, leaf_average=False)\n",
    "mdi_test_values = mdi_explainer.explain_linear_partial(X_test, y=None, l2norm = True, sign = True, leaf_average=False)\n",
    "baseline_train_values = baseline_explainer.explain_linear_partial(X=X_train, y=y_train)\n",
    "baseline_test_values = baseline_explainer.explain_linear_partial(X=X_test, y=None)\n",
    "\n",
    "# get feature rankings for train and test sets\n",
    "mdi_train_rankings = mdi_explainer.get_rankings(mdi_train_values)\n",
    "mdi_test_rankings = mdi_explainer.get_rankings(mdi_test_values)\n",
    "baseline_train_rankings = baseline_explainer.get_rankings(baseline_train_values)\n",
    "baseline_test_rankings = baseline_explainer.get_rankings(baseline_test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (4320, 20); Test Data Shape: (1852, 20)\n",
      "Number of unique rows of MDI+ training values: (1738, 20)\n",
      "Number of unique rows of baseline MDI+ training values: (1737, 20)\n",
      "Number of unique rows of MDI+ test values: (1738, 20)\n",
      "Number of unique rows of baseline MDI+ test values: (1737, 20)\n",
      "Number of unique rows of MDI+ training rankings: (4318, 20)\n",
      "Number of unique rows of baseline MDI+ training rankings: (4313, 20)\n",
      "Number of unique rows of MDI+ test rankings: (1671, 20)\n",
      "Number of unique rows of baseline MDI+ test rankings: (1708, 20)\n"
     ]
    }
   ],
   "source": [
    "# get unique number of points for raw feature importance values\n",
    "print(f'Train Data Shape: {X_train.shape}; Test Data Shape: {X_test.shape}')\n",
    "\n",
    "print(f'Number of unique rows of MDI+ training values: {np.unique(mdi_test_values, axis=0, return_counts=True)[0].shape}')\n",
    "print(f'Number of unique rows of baseline MDI+ training values: {np.unique(baseline_test_values, axis=0, return_counts=True)[0].shape}')\n",
    "\n",
    "print(f'Number of unique rows of MDI+ test values: {np.unique(mdi_test_values, axis=0, return_counts=True)[0].shape}')\n",
    "print(f'Number of unique rows of baseline MDI+ test values: {np.unique(baseline_test_values, axis=0, return_counts=True)[0].shape}')\n",
    "\n",
    "# get unique number of points for feature rankings\n",
    "print(f'Number of unique rows of MDI+ training rankings: {np.unique(mdi_train_rankings, axis=0, return_counts=True)[0].shape}')\n",
    "print(f'Number of unique rows of baseline MDI+ training rankings: {np.unique(baseline_train_rankings, axis=0, return_counts=True)[0].shape}')\n",
    "\n",
    "print(f'Number of unique rows of MDI+ test rankings: {np.unique(mdi_test_rankings, axis=0, return_counts=True)[0].shape}')\n",
    "print(f'Number of unique rows of baseline MDI+ test rankings: {np.unique(baseline_test_rankings, axis=0, return_counts=True)[0].shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
