{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9866cdbb-01ec-4373-aad2-241ad34f0fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# miscilaneous models\n",
    "import openml\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import os\n",
    "\n",
    "# import warnings\n",
    "# warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7bbe1-90fa-45f0-b52c-8acd78b7fe7e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743e798a-41fe-47a6-aee1-7ed9d5993cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(X, y, cat_feat, num_feat):\n",
    "    # one-hot encode categorical features\n",
    "    X_processed = pd.get_dummies(X, columns=cat_feat, drop_first=True, dtype=int)\n",
    "\n",
    "    # save categorical features after one-hot encoding\n",
    "    cat_feat_dummy = X_processed.drop(columns=num_feat).columns.to_numpy()\n",
    "\n",
    "    # get feature importance\n",
    "    # random forest\n",
    "    imp_model_rf = RandomForestRegressor(\n",
    "        min_samples_leaf=5, max_features=0.33, n_estimators=100, random_state=777\n",
    "    )\n",
    "    feat_imp_rf = imp_model_rf.fit(X_processed, y).feature_importances_\n",
    "\n",
    "    imp_df = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": X_processed.columns,\n",
    "            \"importance_rf\": feat_imp_rf,\n",
    "        }\n",
    "    ).sort_values(\"importance_rf\", ascending=False)\n",
    "\n",
    "    X_binned = X_processed[num_feat].apply(\n",
    "        lambda c: pd.qcut(c, q=4, duplicates=\"drop\"), axis=0\n",
    "    )\n",
    "    X_binned = pd.concat([X_binned, X_processed[cat_feat_dummy]], axis=1)\n",
    "\n",
    "    subgroup_dict = {\n",
    "        \"num_feat\": num_feat,\n",
    "        \"cat_feat\": cat_feat_dummy,\n",
    "        \"importance\": imp_df,\n",
    "        \"binned_df\": X_binned,\n",
    "    }\n",
    "\n",
    "    return X_processed, y, subgroup_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f9a5f-26e0-4f72-8e74-adba1fe95365",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Parkinsons\n",
    "\n",
    "[Parkinsons Telemonitoring](https://archive.ics.uci.edu/dataset/189/parkinsons+telemonitoring)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "This dataset is composed of a range of biomedical voice measurements from 42 people with early-stage Parkinson's disease recruited to a six-month trial of a telemonitoring device for remote symptom progression monitoring. The recordings were automatically captured in the patient's homes.\n",
    "\n",
    "#### Introductory Paper:\n",
    "\n",
    "[Accurate Telemonitoring of Parkinson's Disease Progression by Noninvasive Speech Tests](https://www.semanticscholar.org/paper/Accurate-Telemonitoring-of-Parkinson's-Disease-by-Tsanas-Little/1fdf33b6d8b1bdb38866ba824c1dcaecdfb6bdd6)\n",
    "\n",
    "By A. Tsanas, Max A. Little, P. McSharry, L. Ramig, 2009\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54213707-6064-477c-bac7-dd33621e94fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_parkinsons\"\n",
    "# fetch dataset\n",
    "parkinsons_telemonitoring = fetch_ucirepo(id=189)\n",
    "\n",
    "X_orig = parkinsons_telemonitoring.data.features.drop(columns=\"test_time\")\n",
    "y = parkinsons_telemonitoring.data.targets.total_UPDRS.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_parkinsons = [\"sex\"]\n",
    "num_feat_parkinsons = X_orig.drop(columns=cat_feat_parkinsons).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_parkinsons, num_feat_parkinsons)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "new_bin_df = bin_df.copy(deep=True)\n",
    "new_bin_df[\"DFA\"] = (X.DFA <= 0.68).astype(int)\n",
    "subgroups[\"new_binned_df\"] = new_bin_df\n",
    "\n",
    "# if not os.path.exists(f\"data/{data}\"):\n",
    "#     os.mkdir(f\"data/{data}\")\n",
    "\n",
    "# bin_df_path = f\"data/{data}/bin_df.csv\"\n",
    "# X_path = f\"data/{data}/X.csv\"\n",
    "# y_path = f\"data/{data}/y.csv\"\n",
    "\n",
    "# # Save dataframes or arrays\n",
    "# bin_df.to_csv(bin_df_path, index=False)\n",
    "# X.to_csv(X_path, index=False)\n",
    "# np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c4b166-36a9-4219-8a47-2538b893698b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                int64\n",
       "Jitter(%)        float64\n",
       "Jitter(Abs)      float64\n",
       "Jitter:RAP       float64\n",
       "Jitter:PPQ5      float64\n",
       "Jitter:DDP       float64\n",
       "Shimmer          float64\n",
       "Shimmer(dB)      float64\n",
       "Shimmer:APQ3     float64\n",
       "Shimmer:APQ5     float64\n",
       "Shimmer:APQ11    float64\n",
       "Shimmer:DDA      float64\n",
       "NHR              float64\n",
       "HNR              float64\n",
       "RPDE             float64\n",
       "DFA              float64\n",
       "PPE              float64\n",
       "sex_1              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get column types in X\n",
    "col_types = X.dtypes\n",
    "col_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1a1c05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.2000e+01, 6.6200e-03, 3.3800e-05, ..., 5.4842e-01, 1.6006e-01,\n",
       "        0.0000e+00],\n",
       "       [7.2000e+01, 3.0000e-03, 1.6800e-05, ..., 5.6477e-01, 1.0810e-01,\n",
       "        0.0000e+00],\n",
       "       [7.2000e+01, 4.8100e-03, 2.4600e-05, ..., 5.4405e-01, 2.1014e-01,\n",
       "        0.0000e+00],\n",
       "       ...,\n",
       "       [6.1000e+01, 3.4900e-03, 2.4700e-05, ..., 5.7888e-01, 1.4157e-01,\n",
       "        0.0000e+00],\n",
       "       [6.1000e+01, 2.8100e-03, 2.0300e-05, ..., 5.6327e-01, 1.4204e-01,\n",
       "        0.0000e+00],\n",
       "       [6.1000e+01, 2.8200e-03, 2.1100e-05, ..., 5.7077e-01, 1.5336e-01,\n",
       "        0.0000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data\n",
    "from os.path import join as oj\n",
    "dir_data = \"data/data_parkinsons\"\n",
    "X = pd.read_csv(oj(dir_data, \"X.csv\"))\n",
    "# X = np.loadtxt(oj(dir_data, \"X.csv\"), delimiter=\",\")\n",
    "# y = np.loadtxt(oj(dir_data, \"y.csv\"), delimiter=\",\")\n",
    "X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d6a0a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34.894, 35.389, 35.81 , ..., 32.495, 32.007, 31.513])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(oj(dir_data, \"y.csv\")).to_numpy().flatten()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c477f796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5870</th>\n",
       "      <td>33.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5871</th>\n",
       "      <td>32.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>32.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>32.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>31.513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5875 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0     34.398\n",
       "1     34.894\n",
       "2     35.389\n",
       "3     35.810\n",
       "4     36.375\n",
       "...      ...\n",
       "5870  33.485\n",
       "5871  32.988\n",
       "5872  32.495\n",
       "5873  32.007\n",
       "5874  31.513\n",
       "\n",
       "[5875 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(oj(dir_data, \"y.csv\"), header=None)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11135d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5875, 18)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c19e0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5874,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f4972-d0d7-4635-8c44-5c85bffe8300",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Airfoil\n",
    "\n",
    "[Airfoil Self-Noise - UCI](https://archive.ics.uci.edu/dataset/291/airfoil+self+noise)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "NASA data set, obtained from a series of aerodynamic and acoustic tests of two and three-dimensional airfoil blade sections conducted in an anechoic wind tunnel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "642eef93-bad8-4214-813f-a50de6e015dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_airfoil\"\n",
    "# fetch dataset\n",
    "airfoil_self_noise = fetch_ucirepo(id=291)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X_orig = airfoil_self_noise.data.features\n",
    "y = airfoil_self_noise.data.targets[\"scaled-sound-pressure\"].to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_airfoil = []\n",
    "num_feat_airfoil = X_orig.drop(columns=cat_feat_airfoil).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_airfoil, num_feat_airfoil)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabce3f1",
   "metadata": {},
   "source": [
    "# California Housing\n",
    "\n",
    "[California Housing Prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices/data)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "This is the dataset used in the second chapter of Aurélien Géron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understandable list of variables and sits at an optimal size between being to toyish and too cumbersome.\n",
    "\n",
    "The data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6acb392",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_cal_housing\"\n",
    "\n",
    "# fetch data\n",
    "housing = pd.read_csv(\"../data/data_cal_housing/cal_housing.data\", delimiter=\",\", names = [\"lon\", \"lat\", \"med_age\", \"total_rooms\", \"total_beds\", \"population\", \"households\", \"med_income\", \"med_price\"])\n",
    "\n",
    "# data\n",
    "X_orig = housing.drop(columns=\"med_price\")\n",
    "y = housing.med_price.to_numpy()\n",
    "\n",
    "X_orig, y = resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "X_orig = X_orig.reset_index(drop=True)\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_ca_housing = []\n",
    "num_feat_ca_housing = X_orig.drop(columns = cat_feat_ca_housing).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_ca_housing, num_feat_ca_housing)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9bf21b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching california_housing from sklearn\n"
     ]
    }
   ],
   "source": [
    "# get california housing from imodels\n",
    "from imodels import get_clean_dataset\n",
    "X, y, colnames = get_clean_dataset(\"california_housing\", data_source=\"sklearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cba0f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9db5305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "X.to_csv(\"data/data_california/X.csv\", index=False)\n",
    "np.savetxt(\"data/data_california/y.csv\", y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f85f10-296e-48de-b1a8-6d2bdd84acec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Computer\n",
    "\n",
    "[cpu_act - OpenML](https://www.openml.org/search?type=data&status=active&id=197&sort=runs)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "The Computer Activity databases are a collection of computer systems activity measures. The data was collected from a Sun Sparcstation 20/712 with 128 Mbytes of memory running in a multi-user university department. Users would typically be doing a large variety of tasks ranging from accessing the internet, editing files or running very cpu-bound programs. The data was collected continuously on two separate occasions. On both occassions, system activity was gathered every 5 seconds. The final dataset is taken from both occasions with equal numbers of observations coming from each collection epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb2f096-3e01-48ac-b783-49aa8066ed06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_computer\"\n",
    "\n",
    "# fetch dataset\n",
    "computer = openml.datasets.get_dataset(197)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = computer.get_data(\n",
    "    target=computer.default_target_attribute, dataset_format=\"dataframe\"\n",
    ")\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_computer = []\n",
    "num_feat_computer = X_orig.drop(columns=cat_feat_computer).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_computer, num_feat_computer)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dca977-6211-4d67-a13a-b2015fc3d57b",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Concrete\n",
    "\n",
    "[Concrete Compressive Strength - UCI](https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "efb9a6a2-382c-43cf-85c5-7e9c63a1f857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_concrete\"\n",
    "\n",
    "# fetch dataset\n",
    "concrete_compressive_strength = fetch_ucirepo(id=165)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X_orig = concrete_compressive_strength.data.features\n",
    "y = concrete_compressive_strength.data.targets[\n",
    "    \"Concrete compressive strength\"\n",
    "].to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_concrete = []\n",
    "num_feat_concrete = X_orig.drop(columns=cat_feat_concrete).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_concrete, num_feat_concrete)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52eb22-2114-42c3-858f-8bc7d9cfefbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Powerplant\n",
    "\n",
    "[Combined Cycle Power Plant - UCI](https://archive.ics.uci.edu/dataset/294/combined+cycle+power+plant)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "The dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the plant was set to work with full load.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a375f465-3bf1-4dbe-aaca-4b645194a0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_powerplant\"\n",
    "\n",
    "# fetch dataset\n",
    "combined_cycle_power_plant = fetch_ucirepo(id=294)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X_orig = combined_cycle_power_plant.data.features\n",
    "y = combined_cycle_power_plant.data.targets.PE.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_powerplant = []\n",
    "num_feat_powerplant = X_orig.drop(columns=cat_feat_powerplant).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_powerplant, num_feat_powerplant)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6423b3f-7917-49d7-b5b5-e9b6ff6af32e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Miami Housing\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "The dataset contains information on 13,932 single-family homes sold in Miami .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3387fcd-9d85-40a8-8ceb-eaf69e3e328a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3148554/1026814763.py:4: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  miami_housing = openml.datasets.get_dataset(43093)\n"
     ]
    }
   ],
   "source": [
    "data = \"data_miami_housing\"\n",
    "\n",
    "# fetch dataset\n",
    "miami_housing = openml.datasets.get_dataset(43093)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = miami_housing.get_data(\n",
    "    target=miami_housing.default_target_attribute, dataset_format=\"dataframe\"\n",
    ")\n",
    "X_orig, y = resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "X_orig = X_orig.drop(columns=\"PARCELNO\").reset_index(drop=True)\n",
    "y = y.to_numpy()\n",
    "\n",
    "# log-transform target\n",
    "y = np.log(y)\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_miami_housing = [\"avno60plus\", \"month_sold\", \"structure_quality\"]\n",
    "num_feat_miami_housing = X_orig.drop(columns=cat_feat_miami_housing).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(\n",
    "    X_orig, y, cat_feat_miami_housing, num_feat_miami_housing\n",
    ")\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "# if not os.path.exists(f\"../data/{data}\"):\n",
    "#     os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "# bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "# X_path = f\"../data/{data}/X.csv\"\n",
    "# y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# # Save dataframes or arrays\n",
    "# bin_df.to_csv(bin_df_path, index=False)\n",
    "# X.to_csv(X_path, index=False)\n",
    "# np.savetxt(y_path, y, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11618e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write to csv\n",
    "# X.to_csv(\"data/data_miami/X.csv\", index=False)\n",
    "# np.savetxt(\"data/data_miami/y.csv\", y, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255db2f6-4d60-45a3-9811-363de506e4d4",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Insurance\n",
    "\n",
    "[Insurance - pycaret](https://github.com/pycaret/pycaret/blob/master/datasets/insurance.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e40c2b68-2f78-4405-9d09-2e31e18068db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_insurance\"\n",
    "\n",
    "# fetch data\n",
    "insurance = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/pycaret/datasets/main/data/common/insurance.csv\"\n",
    ")\n",
    "\n",
    "# data\n",
    "X_orig = insurance.drop(columns=\"charges\")\n",
    "y = insurance.charges.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_insurance = [\"sex\", \"smoker\", \"region\"]\n",
    "num_feat_insurance = X_orig.drop(columns=cat_feat_insurance).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_insurance, num_feat_insurance)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50ecc3-c7dc-4a22-a6df-a18305854911",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# QSAR\n",
    "\n",
    "[QSAR - openML](https://www.openml.org/search?type=data&sort=runs&id=33368)\n",
    "\n",
    "#### Dataset Information\n",
    "\n",
    "This dataset contains QSAR data (from ChEMBL version 17) showing activity values (unit is pseudo-pCI50) of several compounds on drug target ChEMBL_ID: CHEMBL205 (TID: 15), and it has 3666 rows and 72 features (not including molecule IDs and class feature: molecule_id and pXC50). The features represent Molecular Descriptors which were generated from SMILES strings. Missing value imputation was applied to this dataset (By choosing the Median). Feature selection was also applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3cd0678d-a73f-47c2-a7b9-cedc44c38bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_qsar\"\n",
    "\n",
    "# fetch data\n",
    "qsar = openml.tasks.get_task(360932)\n",
    "\n",
    "# data\n",
    "X_orig, y = qsar.get_X_and_y(dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()\n",
    "\n",
    "# select features with variance\n",
    "k = 500\n",
    "X_array = X_orig.to_numpy()\n",
    "top_k_columns = X_orig.columns[np.argsort(np.var(X_array, axis=0))[-k:]]\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "X_orig, y = resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_qsar = X_orig.columns.tolist()\n",
    "num_feat_qsar = X_orig.drop(columns=cat_feat_qsar).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_qsar, num_feat_qsar)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d923f-1c8a-4911-8a21-529f61fc1ea0",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Allstate\n",
    "\n",
    "[Allstate Insurance - openML](https://api.openml.org/d/45064)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "866c0362-8339-4471-a36d-f7c9d96664ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_allstate\"\n",
    "\n",
    "# fetch dataset\n",
    "allstate = openml.datasets.get_dataset(42571)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = allstate.get_data(\n",
    "    target=allstate.default_target_attribute, dataset_format=\"dataframe\"\n",
    ")\n",
    "y = y.to_numpy()\n",
    "\n",
    "X_orig, y = resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_allstate = np.array(col_names)[cat_ind].tolist()\n",
    "num_feat_allstate = X_orig.drop(columns=cat_feat_allstate).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_allstate, num_feat_allstate)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe708459-9f0c-4bbc-8456-c627f85abd80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Mercedes\n",
    "\n",
    "[Mercedes_Benz_Greener_Manufacturing - OpenML](https://www.openml.org/search?type=data&sort=version&status=any&order=asc&exact_name=Mercedes_Benz_Greener_Manufacturing)\n",
    "\n",
    "#### Dataset Information\n",
    "\n",
    "Datasets provide training data for machine learning models. OpenML datasets are uniformly formatted and come with rich meta-data to allow automated processing. You can sort or filter them by a range of different properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f38b61e-17ce-492b-9e1a-b77372051d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_mercedes\"\n",
    "\n",
    "# fetch dataset\n",
    "mercedes = openml.datasets.get_dataset(42570)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = mercedes.get_data(\n",
    "    target=mercedes.default_target_attribute, dataset_format=\"dataframe\"\n",
    ")\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_mercedes = X_orig.columns.tolist()\n",
    "num_feat_mercedes = X_orig.drop(columns=cat_feat_mercedes).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_mercedes, num_feat_mercedes)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d6a06-e922-4d94-a429-746b05603361",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Transaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "baa3dd3c-de63-44e4-8ef0-2542646805b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_transaction\"\n",
    "\n",
    "# fetch dataset\n",
    "transaction = openml.datasets.get_dataset(42572)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = transaction.get_data(\n",
    "    target=transaction.default_target_attribute, dataset_format=\"dataframe\"\n",
    ")\n",
    "\n",
    "# select features with variance\n",
    "k = 500\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_transaction = []\n",
    "num_feat_transaction = X_orig.drop(columns=cat_feat_transaction).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_transaction, num_feat_transaction)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859b75a",
   "metadata": {},
   "source": [
    "# Energy Efficiency\n",
    "\n",
    "[Energy Efficiency - UCI](https://archive.ics.uci.edu/dataset/242/energy+efficiency)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "This study looked into assessing the heating load and cooling load requirements of buildings (that is, energy efficiency) as a function of building parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf35316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_energy_efficiency\"\n",
    "energy_efficiency = pd.read_csv(\"../data/data_energy_efficiency/energy_efficiency.csv\")\n",
    "\n",
    "energy_efficiency.columns = [\n",
    "    'Relative_Compactness',\n",
    "    'Surface_Area',\n",
    "    'Wall_Area',\n",
    "    'Roof_Area',\n",
    "    'Overall_Height',\n",
    "    'Orientation',\n",
    "    'Glazing_Area',\n",
    "    'Glazing_Area_Distribution',\n",
    "    'Heating_Load',\n",
    "    'Cooling_Load'\n",
    "]\n",
    "\n",
    "X_orig = energy_efficiency.drop(columns=['Heating_Load', 'Cooling_Load'])\n",
    "y_1 = energy_efficiency[['Heating_Load']].values.ravel()\n",
    "y_2 = energy_efficiency[['Cooling_Load']].values.ravel()\n",
    "# save categorical and numerical features\n",
    "cat_feat = ['Orientation']\n",
    "num_feat = X_orig.drop(columns=cat_feat).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y_1, cat_feat, num_feat)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb374b2",
   "metadata": {},
   "source": [
    "# kin8nm\n",
    "\n",
    "[kin8nm - OpenML](https://www.openml.org/search?type=data&sort=runs&id=189&status=active)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "This is data set is concerned with the forward kinematics of an 8 link robot arm. Among the existing variants of this data set we have used the variant 8nm, which is known to be highly non-linear and medium noisy.\n",
    "\n",
    "Original source: DELVE repository of data. Source: collection of regression datasets by Luis Torgo (ltorgo@ncc.up.pt) at http://www.ncc.up.pt/~ltorgo/Regression/DataSets.html Characteristics: 8192 cases, 9 attributes (0 nominal, 9 continuous).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669458cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_kin8nm\"\n",
    "kin8nm = openml.datasets.get_dataset(189)\n",
    "X_orig, y, _, _ = kin8nm.get_data(target=kin8nm.default_target_attribute)\n",
    "\n",
    "cat_feat = []\n",
    "num_feat = X_orig.drop(columns=cat_feat).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat, num_feat)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156bd12a",
   "metadata": {},
   "source": [
    "# Protein Structure\n",
    "\n",
    "[Protein Structure - UCI](https://archive.ics.uci.edu/dataset/265/physicochemical+properties+of+protein+tertiary+structure)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "This is a data set of Physicochemical Properties of Protein Tertiary Structure. The data set is taken from CASP 5-9. There are 45730 decoys and size varying from 0 to 21 armstrong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c166a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_protein_structure\"\n",
    "protein_structure = pd.read_csv(\"../data/data_protein_structure/protein_structure.csv\")\n",
    "\n",
    "column_names = ['RMSD', 'F1_Total_surface_area', 'F2_Non_polar_exposed_area', \n",
    "                    'F3_Fractional_area_of_exposed_non_polar_residue', \n",
    "                    'F4_Fractional_area_of_exposed_non_polar_part_of_residue',\n",
    "                    'F5_Molecular_mass_weighted_exposed_area', \n",
    "                    'F6_Average_deviation_from_standard_exposed_area_of_residue',\n",
    "                    'F7_Euclidean_distance', 'F8_Secondary_structure_penalty', \n",
    "                    'F9_Spatial_Distribution_constraints']\n",
    "\n",
    "protein_structure.columns = column_names\n",
    "\n",
    "X_orig = protein_structure.drop(columns=['RMSD'])\n",
    "y = protein_structure[['RMSD']].values.ravel()\n",
    "\n",
    "cat_feat = []\n",
    "num_feat = X_orig.drop(columns=cat_feat).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat, num_feat)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "new_bin_df = bin_df.copy(deep=True)\n",
    "new_bin_df[\"DFA\"] = (X.F6_Average_deviation_from_standard_exposed_area_of_residue <= 181.87).astype(int)\n",
    "subgroups[\"new_binned_df\"] = new_bin_df\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0025f7",
   "metadata": {},
   "source": [
    "# Naval Propulsion\n",
    "\n",
    "[Naval Propulsion - Kaggle](https://archive.ics.uci.edu/dataset/316/condition+based+maintenance+of+naval+propulsion+plants)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "Data have been generated from a sophisticated simulator of a Gas Turbines (GT), mounted on a Frigate characterized by a COmbined Diesel eLectric And Gas (CODLAG) propulsion plant type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d39c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data_naval_propulsion'\n",
    "\n",
    "column_names = ['Lever_position', 'Ship_speed', 'Gas_Turbine_shaft_torque', \n",
    "                'Gas_Turbine_rate_of_revolutions', 'Gas_Generator_rate_of_revolutions',\n",
    "                'Starboard_Propeller_Torque', 'Port_Propeller_Torque', \n",
    "                'HP_Turbine_exit_temperature', 'GT_Compressor_inlet_air_temperature', \n",
    "                'GT_Compressor_outlet_air_temperature', 'HP_Turbine_exit_pressure',\n",
    "                'GT_Compressor_inlet_air_pressure', 'GT_Compressor_outlet_air_pressure', \n",
    "                'Gas_Turbine_exhaust_gas_pressure', 'Turbine_Injection_Control',\n",
    "                'Fuel_flow', 'GT_Compressor_decay_state_coefficient', 'GT_Turbine_decay_state_coefficient']\n",
    "\n",
    "naval_propulsion = pd.read_csv(\"../data/data_naval_propulsion/naval_propulsion.txt\", delim_whitespace=True, header=None, names=column_names)\n",
    "\n",
    "X_orig = naval_propulsion.drop(columns=['Fuel_flow'])\n",
    "y = naval_propulsion[['Fuel_flow']].values.ravel()\n",
    "# save categorical and numerical features\n",
    "cat_feat = ['Ship_speed']\n",
    "num_feat = X_orig.drop(columns=cat_feat).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat, num_feat)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c89015",
   "metadata": {},
   "source": [
    "# Superconductor\n",
    "\n",
    "[superconduct - OpenML](https://openml.org/search?type=data&sort=version&status=any&order=asc&exact_name=superconduct&id=44148)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "Dataset used in the tabular data benchmark https://github.com/LeoGrin/tabular-benchmark, transformed in the same way. This dataset belongs to the \"regression on numerical features\" benchmark. Original description:\n",
    "\n",
    "The data contains information on 21263 superconductors. The first 81 columns contain extracted features and the 82nd column contains the critical temperature which is used as the target variable. The original data from which the features were extracted comes from http://supercon.nims.go.jp/index_en.html, which is public.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be7d9b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Received uncompressed content from OpenML for https://api.openml.org/data/v1/download/22103273/superconduct.arff.\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='openml1.win.tue.nl', port=443): Read timed out. (read timeout=300)\")': /datasets/0004/44148/dataset_44148.pq\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='openml1.win.tue.nl', port=443): Read timed out. (read timeout=300)\")': /datasets/0004/44148/dataset_44148.pq\n",
      "WARNING:openml.datasets.functions:Could not download file from https://openml1.win.tue.nl/datasets/0004/44148/dataset_44148.pq: HTTPSConnectionPool(host='openml1.win.tue.nl', port=443): Max retries exceeded with url: /datasets/0004/44148/dataset_44148.pq (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n",
      "WARNING:openml.datasets.functions:Failed to download parquet, fallback on ARFF.\n"
     ]
    }
   ],
   "source": [
    "data = \"data_superconductor\"\n",
    "\n",
    "# fetch dataset\n",
    "superconductor = openml.datasets.get_dataset(44148)\n",
    "\n",
    "data\n",
    "X_orig, y, cat_ind, col_names = superconductor.get_data(target=superconductor.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_superconductor = []\n",
    "num_feat_superconductor = X_orig.drop(columns = cat_feat_superconductor).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_superconductor, num_feat_superconductor)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "X_path = f\"../data/{data}/X.csv\"\n",
    "y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# Save dataframes or arrays\n",
    "bin_df.to_csv(bin_df_path, index=False)\n",
    "X.to_csv(X_path, index=False)\n",
    "np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a93e9",
   "metadata": {},
   "source": [
    "# Diamond\n",
    "\n",
    "[Diamond - OpenML](https://openml.org/search?type=data&sort=version&status=any&order=asc&exact_name=diamonds&id=42225)\n",
    "\n",
    "#### Dataset Information:\n",
    "\n",
    "This classic dataset contains the prices and other attributes of almost 54,000 diamonds. It's a great dataset for beginners learning to work with data analysis and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3178394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3148554/55634978.py:4: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  diamond = openml.datasets.get_dataset(42225)\n"
     ]
    }
   ],
   "source": [
    "data = \"data_diamond\"\n",
    "\n",
    "# fetch dataset\n",
    "diamond = openml.datasets.get_dataset(42225)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = diamond.get_data(target=diamond.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()\n",
    "\n",
    "# subsample data\n",
    "X_orig, y = resample(X_orig, y, replace=False, n_samples=7000, random_state=777)\n",
    "X_orig = X_orig.reset_index(drop=True)\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_diamond = np.array(col_names)[cat_ind].tolist()\n",
    "num_feat_diamond = X_orig.drop(columns = cat_feat_diamond).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_diamond, num_feat_diamond)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "# if not os.path.exists(f\"../data/{data}\"):\n",
    "#     os.mkdir(f\"../data/{data}\")\n",
    "\n",
    "# bin_df_path = f\"../data/{data}/bin_df.csv\"\n",
    "# X_path = f\"../data/{data}/X.csv\"\n",
    "# y_path = f\"../data/{data}/y.csv\"\n",
    "\n",
    "# # Save dataframes or arrays\n",
    "# bin_df.to_csv(bin_df_path, index=False)\n",
    "# X.to_csv(X_path, index=False)\n",
    "# np.savetxt(y_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce259a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "X.to_csv(\"data/data_diamond/X.csv\", index=False)\n",
    "np.savetxt(\"data/data_diamond/y.csv\", y, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
