{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/binyu/zhongyuan_liang/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "import imodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from imodels.tree.rf_plus.rf_plus.rf_plus_models import RandomForestPlusRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, mean_squared_error, r2_score, average_precision_score\n",
    "from imodels.tree.rf_plus.feature_importance.rfplus_explainer import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from feature_importance.scripts.simulations_util import *\n",
    "from scripts.competing_methods_local import *\n",
    "from rbo_implementation import rbo_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_fi_derivation(X, support, dgp):\n",
    "    fi = np.zeros_like(X)  # Initialize feature importance array\n",
    "    \n",
    "    if dgp == \"linear\":\n",
    "        fi = np.abs(X)  # Use absolute values for linear case\n",
    "        fi[:, support == 0] = 0  # Set non-supported features to 0\n",
    "    \n",
    "    elif dgp == \"polynomial\":\n",
    "        for j in range(X.shape[1]):\n",
    "            if support[j] == 1:\n",
    "                if j in [0, 2, 4]:\n",
    "                    fi[:, j] = np.abs(X[:, j] + X[:, j] * X[:, j + 1])\n",
    "                else:\n",
    "                    fi[:, j] = np.abs(X[:, j] * X[:, j - 1])\n",
    "    \n",
    "    elif dgp == \"lss\":\n",
    "        for j in range(X.shape[1]):\n",
    "            if support[j] == 1:\n",
    "                if j in [0, 2, 4]:\n",
    "                    fi[:, j] = np.abs((X[:, j] > 0) * (X[:, j + 1] > 0) - 0.5 * (X[:, j + 1] > 0))\n",
    "                else:\n",
    "                    fi[:, j] = np.abs((X[:, j] > 0) * (X[:, j - 1] > 0) - 0.5 * (X[:, j - 1] > 0))\n",
    "    \n",
    "    elif dgp == \"linear_lss\":\n",
    "        for j in range(X.shape[1]):\n",
    "            if support[j] == 1:\n",
    "                if j in [0, 2, 4]:\n",
    "                    fi[:, j] = np.abs(X[:, j] + X[:, j] * X[:, j + 1] + ((X[:, j] > 0) * (X[:, j + 1] > 0) - 0.5 * (X[:, j + 1] > 0)))\n",
    "                else:\n",
    "                    fi[:, j] = np.abs(X[:, j] + ((X[:, j] > 0) * (X[:, j - 1] > 0) - 0.5 * (X[:, j - 1] > 0)))\n",
    "    return fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debug the differences yielded by AUROC and RBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_642164/3596219487.py:1: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  X = sample_real_data_X(source=\"openml\", task_id = 3917, normalize=True)\n",
      "/scratch/users/zhongyuan_liang/conda/envs/mdi/lib/python3.10/site-packages/openml/tasks/functions.py:442: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  dataset = get_dataset(task.dataset_id, *dataset_args, **get_dataset_kwargs)\n",
      "/scratch/users/zhongyuan_liang/conda/envs/mdi/lib/python3.10/site-packages/openml/tasks/task.py:150: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  return datasets.get_dataset(self.dataset_id)\n",
      "/accounts/projects/binyu/zhongyuan_liang/local_MDI+/imodels-experiments/feature_importance/../feature_importance/scripts/simulations_util.py:61: FutureWarning: Support for `dataset_format='array'` will be removed in 0.15,start using `dataset_format='dataframe' to ensure your code will continue to work. You can use the dataframe's `to_numpy` function to continue using numpy arrays.\n",
      "  X, _, _, _ = dataset.get_data(target=dataset.default_target_attribute,dataset_format=\"array\")\n"
     ]
    }
   ],
   "source": [
    "X = sample_real_data_X(source=\"openml\", task_id = 3917, normalize=True)\n",
    "# y, support, beta = linear_model(X, sigma=None, s=5, beta=1, heritability=0.999999999999, return_support=True, seed=42)\n",
    "# make y 0/1\n",
    "# y = (y > 0).astype(int)\n",
    "# y, support, beta = lss_model(X, m=3, r=2, beta=1, sigma=None, tau=0.5, heritability=0.99999999, return_support=True)\n",
    "# y, support, beta = hierarchical_poly(X, m=3, r=2, beta=1, heritability=0.999999, return_support=True)\n",
    "#y, support, beta = partial_linear_lss_model(X, s=1, m=3, r=2, beta=1, sigma=None, tau=0.5, heritability=0.99999999, return_support=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, support, _ = logistic_linear_model_random_feature(X, beta=1, s=5, frac_label_corruption=0.15, return_support=True, error_seed=1, feature_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   27.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  5.8min finished\n"
     ]
    }
   ],
   "source": [
    "est = RandomForestClassifier(n_estimators=100, min_samples_leaf=1, max_features='sqrt', random_state=42)\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "rf_plus_base_ridge = RandomForestPlusClassifier(rf_model=est, prediction_model=LogisticRegressionCV(penalty='l2', cv=5, max_iter=10000, random_state=0))\n",
    "rf_plus_base_ridge.fit(X_train, y_train)\n",
    "\n",
    "rf_plus_base_lasso = RandomForestPlusClassifier(rf_model=est, prediction_model=LogisticRegressionCV(penalty='l1', solver = 'saga', cv=5, n_jobs=-1, tol=5e-4, max_iter=5000, random_state=0))\n",
    "rf_plus_base_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LFI_evaluation_MDIRFPlus_all_ranking_retrain(X_train, y_train, X_test, fit=None, mode=\"absolute\"):\n",
    "    assert isinstance(fit, RandomForestPlusRegressor) or isinstance(fit, RandomForestPlusClassifier)\n",
    "    rf_plus_mdi = RFPlusMDI(fit, mode = 'only_k', evaluate_on=\"all\")\n",
    "    local_fi_score_train = rf_plus_mdi.explain_linear_partial(X=X_train, y=y_train, ranking = True)\n",
    "    local_fi_score_test = rf_plus_mdi.explain_linear_partial(X=X_test, y=None, ranking = True)\n",
    "    if mode == \"absolute\":\n",
    "        return np.abs(local_fi_score_train), np.abs(local_fi_score_test)\n",
    "    else:\n",
    "        return local_fi_score_train, local_fi_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_train, ridge_test = LFI_evaluation_MDIRFPlus_all_ranking_retrain(X_train, y_train, X_test, fit=rf_plus_base_ridge, mode=\"absolute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_train, lasso_test = LFI_evaluation_MDIRFPlus_all_ranking_retrain(X_train, y_train, X_test, fit=rf_plus_base_lasso, mode=\"absolute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8321718749999999\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for i in range(ridge_test.shape[0]):\n",
    "    temp.append(roc_auc_score(support, ridge_test[i, :]))\n",
    "print(np.mean(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8668437499999999\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for i in range(lasso_test.shape[0]):\n",
    "    temp.append(roc_auc_score(support, lasso_test[i, :]))\n",
    "print(np.mean(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0045231 ,  1.20691848, -1.588879  , ..., -0.09489214,\n",
       "       -0.83254277,  0.12159551])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_train, lasso_test ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0045231 ,  1.20691848, -1.588879  , ..., -0.09489214,\n",
       "       -0.83254277,  0.12159551])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 57)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.ceil(0.05*len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=0)\n",
    "est = RandomForestClassifier(n_estimators=100, min_samples_leaf=3, max_features='sqrt', random_state=42)\n",
    "est.fit(X_train, y_train)\n",
    "rf_plus_base = RandomForestPlusClassifier(rf_model=est)\n",
    "rf_plus_base.fit(X_train, y_train)\n",
    "# rf_plus_base_oob = RandomForestPlusRegressor(rf_model=est, fit_on=\"oob\")\n",
    "# rf_plus_base_oob.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = RFPlusLime(rf_plus_base)\n",
    "local_fi_score_train_subset = explainer.explain(X_train, X_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_fi_derivation(X, support, dgp):\n",
    "    fi = np.zeros_like(X)\n",
    "    assert dgp == \"linear\"\n",
    "    fi = np.abs(X) \n",
    "    fi[:, support == 0] = 0\n",
    "    return fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ground_truth_fi_derivation(X_train, support, \"linear\")[0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_largest_k(arr, k):\n",
    "    indices = np.argpartition(arr, -k)[-k:]\n",
    "    encoded_array = np.zeros_like(arr)\n",
    "    encoded_array[indices] = 1\n",
    "    return encoded_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_largest_k(temp, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train_subset[:,:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train_subset[:,:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train_subset[:,:,1][1]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(local_fi_score_train_subset[1]),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_plus_kernel_shap = RFPlusKernelSHAP(rf_plus_base)\n",
    "local_fi_score_train = None\n",
    "local_fi_score_train_subset = rf_plus_kernel_shap.explain(X_train=X_train, X_test=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(local_fi_score_train_subset),axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train_subset[:,:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train_subset[:,:,1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alo_mdi = AloRFPlusMDI(rf_plus_base, evaluate_on=\"oob\")\n",
    "rf_plus_lime = RFPlusLime(rf_plus_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train_l2_norm_sign = np.abs(alo_mdi.explain_linear_partial(X=X_train, y=y_train, l2norm=True, sign=True))\n",
    "local_fi_score_train_l2_norm = np.abs(alo_mdi.explain_linear_partial(X=X_train, y=y_train, l2norm=True))\n",
    "local_fi_score_train = np.abs(alo_mdi.explain_linear_partial(X=X_train, y=y_train, l2norm=False))\n",
    "lime_train = np.abs(rf_plus_lime.explain(X_train=X_train, X_test=X_train).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_lmdi_norm_sign= []\n",
    "for i in range(local_fi_score_train.shape[0]):       \n",
    "    auroc_lmdi_norm_sign.append(roc_auc_score([1]*5+[0]*5, local_fi_score_train_l2_norm_sign[i]))\n",
    "print(np.mean(auroc_lmdi_norm_sign))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_lmdi_norm= []\n",
    "for i in range(local_fi_score_train.shape[0]):       \n",
    "    auroc_lmdi_norm.append(roc_auc_score([1]*5+[0]*5, local_fi_score_train_l2_norm[i]))\n",
    "print(np.mean(auroc_lmdi_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_lmdi= []\n",
    "for i in range(local_fi_score_train.shape[0]):       \n",
    "    auroc_lmdi.append(roc_auc_score([1]*5+[0]*5, local_fi_score_train[i]))\n",
    "print(np.mean(auroc_lmdi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_lime= []\n",
    "for i in range(local_fi_score_train.shape[0]):       \n",
    "    auroc_lime.append(roc_auc_score([1]*5+[0]*5, lime_train[i]))\n",
    "print(np.mean(auroc_lime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIME assessment\n",
    "temp_lime = []\n",
    "for i in range(5):\n",
    "    indices_correct = np.argwhere(np.array(auroc_lime) == 1.0).flatten()\n",
    "    indices = np.argwhere((-1 * lime_train).argsort() == i)[:,1][indices_correct]\n",
    "    values = X_train[indices_correct][np.arange(indices_correct.shape[0]), indices]\n",
    "    mean_abs_values = np.mean(np.abs(values))\n",
    "    temp_lime.append(mean_abs_values)\n",
    "print(temp_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_lmdi = []\n",
    "for i in range(5):\n",
    "    indices_correct = np.argwhere(np.array(auroc_lmdi_norm) == 1.0).flatten()\n",
    "    indices = np.argwhere((-1 * local_fi_score_train_l2_norm).argsort() == i)[:,1][indices_correct]\n",
    "    values = X_train[indices_correct][np.arange(indices_correct.shape[0]), indices]\n",
    "    mean_abs_values = np.mean(np.abs(values))\n",
    "    temp_lmdi.append(mean_abs_values)\n",
    "print(temp_lmdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot temp_lmdi and temp_lime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(temp_lmdi, label=\"lmdi\")\n",
    "plt.plot(temp_lime, label=\"lime\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere((-1 * local_fi_score_train).argsort() == 0)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_correct = np.argwhere(np.array(auroc_lmdi) == 1.0)\n",
    "indices = np.argwhere((-1*local_fi_score_train).argsort() == 0)\n",
    "values = [X_train[tuple(indices[index])] for index in indices_correct]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_correct = np.argwhere(np.array(auroc_lmdi) == 1.0)\n",
    "indices = np.argwhere((-1*local_fi_score_train).argsort() == 1)\n",
    "values = [X_train[tuple(indices[index])] for index in indices_correct]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_correct = np.argwhere(np.array(auroc_lmdi) == 1.0)\n",
    "indices = np.argwhere((-1*local_fi_score_train).argsort() == 2)\n",
    "values = [X_train[tuple(indices[index])] for index in indices_correct]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_correct = np.argwhere(np.array(auroc_lmdi) == 1.0)\n",
    "indices = np.argwhere((-1*local_fi_score_train).argsort() == 3)\n",
    "values = [X_train[tuple(indices[index])] for index in indices_correct]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_correct = np.argwhere(np.array(auroc_lmdi) == 1.0)\n",
    "indices = np.argwhere((-1*local_fi_score_train).argsort() == 4)\n",
    "values = [X_train[tuple(indices[index])] for index in indices_correct]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indices_correct:\n",
    "    print(tuple(indices[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere(np.array(auroc_lmdi) == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere((-1*local_fi_score_train_l2_norm).argsort() == 1)\n",
    "values = [X_train[tuple(index)] for index in indices]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere((-1*local_fi_score_train_l2_norm).argsort() == 2)\n",
    "values = [X_train[tuple(index)] for index in indices]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere((-1*local_fi_score_train_l2_norm).argsort() == 3)\n",
    "values = [X_train[tuple(index)] for index in indices]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere((-1*local_fi_score_train_l2_norm).argsort() == 4)\n",
    "values = [X_train[tuple(index)] for index in indices]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere((-1*lime_train).argsort() == 0)\n",
    "values = [X_train[tuple(index)] for index in indices]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere((-1*lime_train).argsort() == 1)\n",
    "values = [X_train[tuple(index)] for index in indices]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere((-1*lime_train).argsort() == 2)\n",
    "values = [X_train[tuple(index)] for index in indices]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere((-1*lime_train).argsort() == 3)\n",
    "values = [X_train[tuple(index)] for index in indices]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere((-1*lime_train).argsort() == 4)\n",
    "values = [X_train[tuple(index)] for index in indices]\n",
    "np.mean(np.abs(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mean of X_train of all index with 0 in lime_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_plus_mdi_train = AloRFPlusMDI(rf_plus_base_oob, evaluate_on=\"all\")\n",
    "rf_plus_mdi_test = RFPlusMDI(rf_plus_base_oob, evaluate_on=\"all\")\n",
    "local_fi_score_train_lmdi_plus_method2 = np.abs(rf_plus_mdi_train.explain_linear_partial(X=X_train, y=y_train, leaf_average=False))\n",
    "local_fi_score_test_lmdi_plus_method2 = np.abs(rf_plus_mdi_test.explain_linear_partial(X=X_test, y=None))\n",
    "local_fi_score_train_lmdi_plus_method2_l2_norm = np.abs(rf_plus_mdi_train.explain_linear_partial(X=X_train, y=y_train, l2norm=True, leaf_average=False))\n",
    "local_fi_score_test_lmdi_plus_method2_l2_norm = np.abs(rf_plus_mdi_test.explain_linear_partial(X=X_test, y=None, l2norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(est)\n",
    "local_fi_score_train_shap = np.abs(explainer.shap_values(X_train, check_additivity=False))\n",
    "local_fi_score_test_shap = np.abs(explainer.shap_values(X_test, check_additivity=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_shap = []\n",
    "rbo_lst_09_shap = []\n",
    "num_captured_shap = []\n",
    "for i in range(local_fi_score_train_shap.shape[0]):\n",
    "    fi_data_i = local_fi_score_train_shap[i]\n",
    "    ground_truth_fi_i = np.abs(X_train)[i]\n",
    "    ground_truth_fi_i[support == 0] = 0\n",
    "    dict_predictions = dict(enumerate(fi_data_i))\n",
    "    dict_ground_truth = dict(enumerate(ground_truth_fi_i))      \n",
    "    num_signal_features = int(np.sum(support))            \n",
    "    auroc_shap.append(roc_auc_score(support, fi_data_i))\n",
    "    rbo_lst_09_shap.append(rbo_dict(dict1=dict_ground_truth, dict2=dict_predictions, p=0.9)[2])\n",
    "    sorted_indices = np.argsort(-fi_data_i)\n",
    "    top_indices = sorted_indices[:num_signal_features]\n",
    "    actual_signal_features = np.sum(support[top_indices])\n",
    "    num_captured_shap.append(actual_signal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_lmdi_plus= []\n",
    "rbo_lst_09_lmdi_plus = []\n",
    "num_captured_lmdi_plus = []\n",
    "for i in range(local_fi_score_train_lmdi_plus_method2.shape[0]):\n",
    "    fi_data_i = local_fi_score_train_lmdi_plus_method2[i]\n",
    "    ground_truth_fi_i = np.abs(X_train)[i]\n",
    "    ground_truth_fi_i[support == 0] = 0\n",
    "    dict_predictions = dict(enumerate(fi_data_i))\n",
    "    dict_ground_truth = dict(enumerate(ground_truth_fi_i))      \n",
    "    num_signal_features = int(np.sum(support))            \n",
    "    auroc_lmdi_plus.append(roc_auc_score(support, fi_data_i))\n",
    "    rbo_lst_09_lmdi_plus.append(rbo_dict(dict1=dict_ground_truth, dict2=dict_predictions, p=0.9)[2])\n",
    "    sorted_indices = np.argsort(-fi_data_i)\n",
    "    top_indices = sorted_indices[:num_signal_features]\n",
    "    actual_signal_features = np.sum(support[top_indices])\n",
    "    num_captured_lmdi_plus.append(actual_signal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_lmdi_plus_avg= []\n",
    "rbo_lst_09_lmdi_plus_avg = []\n",
    "num_captured_lmdi_plus_avg = []\n",
    "for i in range(local_fi_score_train_lmdi_plus_method2_l2_norm.shape[0]):\n",
    "    fi_data_i = local_fi_score_train_lmdi_plus_method2_l2_norm[i]\n",
    "    ground_truth_fi_i = np.abs(X_train)[i]\n",
    "    ground_truth_fi_i[support == 0] = 0\n",
    "    dict_predictions = dict(enumerate(fi_data_i))\n",
    "    dict_ground_truth = dict(enumerate(ground_truth_fi_i))      \n",
    "    num_signal_features = int(np.sum(support))            \n",
    "    auroc_lmdi_plus_avg.append(roc_auc_score(support, fi_data_i))\n",
    "    rbo_lst_09_lmdi_plus_avg.append(rbo_dict(dict1=dict_ground_truth, dict2=dict_predictions, p=0.9)[2])\n",
    "    sorted_indices = np.argsort(-fi_data_i)\n",
    "    top_indices = sorted_indices[:num_signal_features]\n",
    "    actual_signal_features = np.sum(support[top_indices])\n",
    "    num_captured_lmdi_plus_avg.append(actual_signal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(auroc_shap).mean(), np.array(rbo_lst_09_shap).mean(), np.array(num_captured_shap).mean())\n",
    "print(np.array(auroc_lmdi_plus).mean(), np.array(rbo_lst_09_lmdi_plus).mean(), np.array(num_captured_lmdi_plus).mean())\n",
    "print(np.array(auroc_lmdi_plus_avg).mean(), np.array(rbo_lst_09_lmdi_plus_avg).mean(), np.array(num_captured_lmdi_plus_avg).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_shap[5], rbo_lst_09_shap[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_lmdi_plus[5], rbo_lst_09_lmdi_plus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_fi_i = np.abs(X_test)[5]\n",
    "ground_truth_fi_i[support == 0] = 0\n",
    "ground_truth_fi_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_predictions = dict(enumerate(local_fi_score_test_shap[5]))\n",
    "dict_ground_truth = dict(enumerate(ground_truth_fi_i))\n",
    "rbo_dict(dict1=dict_ground_truth, dict2=dict_predictions, p=p, verbose=True)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_test_lmdi_plus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_predictions = dict(enumerate(local_fi_score_test_lmdi_plus[5]))\n",
    "dict_ground_truth = dict(enumerate(ground_truth_fi_i))\n",
    "rbo_dict(dict1=dict_ground_truth, dict2=dict_predictions, p=p, verbose=True)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array([0.30647695  , 0.17410994, 0.816055, 0.17842848, 0.10012125,\n",
    "       0.26276102, 0.26671546, 0.28039733, 0.23719995, 0.25739759])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_predictions = dict(enumerate(temp))#local_fi_score_test_lmdi_plus[5]))\n",
    "dict_ground_truth = dict(enumerate(ground_truth_fi_i))\n",
    "rbo_dict(dict1=dict_ground_truth, dict2=dict_predictions, p=p, verbose=True)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debug two group setting with intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_normal_X_subgroups(n = 500, d=10, mean= [[0]*10,[0]*5+[0]*5], scale =[[1]*10,[1]*10])\n",
    "temp = linear_model(X, beta=1, sigma=None, heritability=0.6, s=5, return_support=True)\n",
    "y = temp[0]\n",
    "support = temp[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model(X, beta=1, sigma=None, heritability=0.2, s=5, return_support=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, max_features=0.33, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_plus_base = RandomForestPlusRegressor(rf_model=rf)\n",
    "rf_plus_base.fit(X_train, y_train)\n",
    "\n",
    "# rf_plus_base_oob = RandomForestPlusRegressor(rf_model=rf, fit_on=\"oob\")\n",
    "# rf_plus_base_oob.fit(X_train, y_train)\n",
    "\n",
    "# rf_plus_base_inbag = RandomForestPlusRegressor(rf_model=rf, include_raw=False, fit_on=\"inbag\", prediction_model=Ridge(alpha=1e-6))\n",
    "# rf_plus_base_inbag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "if X_train.shape[0] > 100:\n",
    "    indices_train = np.random.choice(X_train.shape[0], 100, replace=False)\n",
    "    X_train_subset = X_train[indices_train]\n",
    "    y_train_subset = y_train[indices_train]\n",
    "else:\n",
    "    indices_train = np.arange(X_train.shape[0])\n",
    "    X_train_subset = X_train\n",
    "    y_train_subset = y_train\n",
    "\n",
    "if X_test.shape[0] > 100:\n",
    "    indices_test = np.random.choice(X_test.shape[0], 100, replace=False)\n",
    "    X_test_subset = X_test[indices_test]\n",
    "    y_test_subset = y_test[indices_test]\n",
    "else:\n",
    "    indices_test = np.arange(X_test.shape[0])\n",
    "    X_test_subset = X_test\n",
    "    y_test_subset = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train_0 = np.where(X_train_subset[:, -1] == 0)[0]\n",
    "indices_test_0 = np.where(X_test_subset[:, -1] == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train_1 = np.where(X_train_subset[:, -1] == 1)[0]\n",
    "indices_test_1 = np.where(X_test_subset[:, -1] == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fi_score_train, local_fi_score_train_subset, local_fi_score_test, local_fi_score_test_subset = tree_shap_evaluation_RF(X_train=X_train, y_train=y_train, X_train_subset = X_train_subset, y_train_subset=y_train_subset,X_test=X_test, y_test=y_test, X_test_subset=X_test_subset, y_test_subset=y_test_subset,fit=rf, mode=\"absolute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_support_train = np.abs(X_train_subset)\n",
    "new_support_test = np.abs(X_test)\n",
    "new_support_train[:, -5:] = 0\n",
    "new_support_test[:, -5:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = local_fi_score_train_subset\n",
    "auroc = []\n",
    "auprc = []\n",
    "for i in range(data.shape[0]):\n",
    "        auroc.append(roc_auc_score(support, data[i]))\n",
    "        auprc.append(average_precision_score(support, data[i]))\n",
    "print(\"Treeshap Trainsubset\")\n",
    "print(np.array(auroc).mean())\n",
    "print(np.array(auprc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = local_fi_score_test \n",
    "auroc = []\n",
    "auprc = []\n",
    "for i in range(data.shape[0]):\n",
    "        auroc.append(roc_auc_score(support, data[i]))\n",
    "        auprc.append(average_precision_score(support, data[i]))\n",
    "print(\"Treeshap Test\")\n",
    "print(np.array(auroc).mean())\n",
    "print(np.array(auprc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_biased_overlap(list1, list2, p=0.9):\n",
    "    \"\"\"\n",
    "    Compute the Rank-Biased Overlap (RBO) between two ranked lists.\n",
    "\n",
    "    Parameters:\n",
    "    - list1: numpy array or list of the first ranked list\n",
    "    - list2: numpy array or list of the second ranked list\n",
    "    - p: the discount factor (default is 0.9, which is commonly used)\n",
    "\n",
    "    Returns:\n",
    "    - rbo: the Rank-Biased Overlap score\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert lists to numpy arrays if they're not already\n",
    "    list1 = np.asarray(list1)\n",
    "    list2 = np.asarray(list2)\n",
    "\n",
    "    # Get the indices that would sort the arrays in descending order\n",
    "    sorted_indices1 = np.argsort(-list1)\n",
    "    sorted_indices2 = np.argsort(-list2)\n",
    "\n",
    "    # Rank lists based on sorted indices\n",
    "    ranked_list1 = sorted_indices1\n",
    "    ranked_list2 = sorted_indices2\n",
    "\n",
    "    # Initialize the overlap\n",
    "    overlap = 0.0\n",
    "    min_len = min(len(ranked_list1), len(ranked_list2))\n",
    "    \n",
    "    # Compute the RBO\n",
    "    for i in range(min_len):\n",
    "        # Calculate the overlap at rank i\n",
    "        rank_i_overlap = len(set(ranked_list1[:i+1]) & set(ranked_list2[:i+1]))\n",
    "        \n",
    "        # Add the discounted overlap to the total\n",
    "        overlap += (rank_i_overlap / (i + 1)) * (p ** (i + 1))\n",
    "    \n",
    "    # Normalize the score\n",
    "    normalization = (1 - p) / (1 - p ** (min_len + 1))\n",
    "    rbo = overlap * normalization\n",
    "    \n",
    "    return rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = local_fi_score_train_subset\n",
    "rbo = []\n",
    "for i in range(data.shape[0]):\n",
    "        rbo.append(rank_biased_overlap(new_support_train[i], data[i]))\n",
    "print(\"Treeshap Trainsubset\")\n",
    "print(np.array(rbo).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = local_fi_score_test\n",
    "rbo = []\n",
    "for i in range(data.shape[0]):\n",
    "        rbo.append(rank_biased_overlap(new_support_test[i], data[i]))\n",
    "print(\"Treeshap Trainsubset\")\n",
    "print(np.array(rbo).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_plus_mdi_train = AloRFPlusMDI(rf_plus_base, evaluate_on=\"oob\")\n",
    "rf_plus_mdi_test = AloRFPlusMDI(rf_plus_base, evaluate_on=\"all\")\n",
    "local_fi_score_train = np.abs(rf_plus_mdi_train.explain_subtract_intercept(X=X_train, y=y_train))\n",
    "local_fi_score_test = np.abs(rf_plus_mdi_test.explain_subtract_intercept(X=X_test, y=None))\n",
    "local_fi_score_test_subset = np.abs(rf_plus_mdi_test.explain_subtract_intercept(X=X_test_subset, y=None))\n",
    "local_fi_score_train_subset = local_fi_score_train[indices_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = local_fi_score_train_subset \n",
    "auroc = []\n",
    "auprc = []\n",
    "for i in range(data.shape[0]):\n",
    "        auroc.append(roc_auc_score(support, data[i]))\n",
    "        auprc.append(average_precision_score(support, data[i]))\n",
    "print(np.array(auroc).mean())\n",
    "print(np.array(auprc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = local_fi_score_test \n",
    "auroc = []\n",
    "auprc = []\n",
    "for i in range(data.shape[0]):\n",
    "        auroc.append(roc_auc_score(support, data[i]))\n",
    "        auprc.append(average_precision_score(support, data[i]))\n",
    "print(np.array(auroc).mean())\n",
    "print(np.array(auprc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_support_train = np.abs(X_train_subset)\n",
    "new_support_test = np.abs(X_test)\n",
    "new_support_train[:, -5:] = 0\n",
    "new_support_test[:, -5:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = local_fi_score_train_subset\n",
    "rbo = []\n",
    "for i in range(data.shape[0]):\n",
    "        rbo.append(rank_biased_overlap(new_support_train[i], data[i]))\n",
    "print(\"Treeshap Trainsubset\")\n",
    "print(np.array(rbo).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = local_fi_score_test\n",
    "rbo_lst = []\n",
    "for i in range(data.shape[0]):\n",
    "        rbo_lst.append(rbo.RankingSimilarity(new_support_test[i], data[i]).rbo())#rbo.append(rank_biased_overlap(new_support_test[i], data[i]))\n",
    "print(\"Treeshap Trainsubset\")\n",
    "print(np.array(rbo).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_support_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbo.RankingSimilarity(S, T).rbo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_plus_mdi_train = AloRFPlusMDI(rf_plus_base, evaluate_on=\"oob\")\n",
    "rf_plus_mdi_test = AloRFPlusMDI(rf_plus_base, evaluate_on=\"all\")\n",
    "local_fi_score_train = np.abs(rf_plus_mdi_train.explain(X=X_train, y=y_train)[1])\n",
    "local_fi_score_test = np.abs(rf_plus_mdi_test.explain(X=X_test, y=None)[1])\n",
    "local_fi_score_test_subset = np.abs(rf_plus_mdi_test.explain(X=X_test_subset, y=None)[1])\n",
    "local_fi_score_train_subset = local_fi_score_train[indices_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = local_fi_score_train_subset \n",
    "auroc = []\n",
    "auprc = []\n",
    "for i in range(data.shape[0]):\n",
    "        auroc.append(roc_auc_score(support, data[i]))\n",
    "        auprc.append(average_precision_score(support, data[i]))\n",
    "print(np.array(auroc).mean())\n",
    "print(np.array(auprc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = local_fi_score_test \n",
    "auroc = []\n",
    "auprc = []\n",
    "for i in range(data.shape[0]):\n",
    "        auroc.append(roc_auc_score(support, data[i]))\n",
    "        auprc.append(average_precision_score(support, data[i]))\n",
    "print(np.array(auroc).mean())\n",
    "print(np.array(auprc).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
