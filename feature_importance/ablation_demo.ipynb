{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from os.path import join as oj\n",
    "import glob\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import time\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from typing import Callable, List, Tuple\n",
    "import itertools\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, mean_squared_error\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../../imodels/\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Bins whose width\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, accuracy_score, roc_auc_score, mean_squared_error\n",
    "\n",
    "from imodels.importance import RandomForestPlusRegressor, RandomForestPlusClassifier, \\\n",
    "    RidgeRegressorPPM, LassoRegressorPPM, IdentityTransformer\n",
    "from imodels.importance.rf_plus import _fast_r2_score\n",
    "import seaborn as sns\n",
    "from util import ModelConfig, FIModelConfig, tp, fp, neg, pos, specificity_score, auroc_score, auprc_score, compute_nsg_feat_corr_w_sig_subspace, apply_splitting_strategy\n",
    "import shap\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_normal_X_subgroups(n, d, mean, scale):\n",
    "    \"\"\"\n",
    "    :param n: Number of samples\n",
    "    :param d: Number of features\n",
    "    :param mean: Nested list of mean of normal distribution for each subgroup\n",
    "    :param scale: Nested ist of scale of normal distribution for each subgroup\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    assert len(mean[0]) == len(scale[0]) == d\n",
    "    num_groups = len(mean)\n",
    "    result = []\n",
    "    group_size = n // num_groups\n",
    "    for i in range(num_groups):\n",
    "        X = np.zeros((group_size, d))\n",
    "        for j in range(d):\n",
    "            X[:, j] = np.random.normal(mean[i][j], scale[i][j], size=group_size)\n",
    "        result.append(X)\n",
    "    return np.vstack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_shuffle(data, seed):\n",
    "    \"\"\"\n",
    "    Randomly shuffle each column of the data.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    return np.array([np.random.permutation(data[:, i]) for i in range(data.shape[1])]).T\n",
    "\n",
    "\n",
    "def ablation(data, feature_importance, mode, num_features, seed):\n",
    "    \"\"\"\n",
    "    Replace the top num_features max feature importance data with random shuffle for each sample\n",
    "    \"\"\"\n",
    "    assert mode in [\"max\", \"min\"]\n",
    "    fi = feature_importance.to_numpy()\n",
    "    shuffle = generate_random_shuffle(data.copy(), seed)\n",
    "    if mode == \"max\":\n",
    "        indices = np.argsort(-fi)\n",
    "    else:\n",
    "        indices = np.argsort(fi)\n",
    "    data_copy = data.copy()\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(num_features):\n",
    "            data_copy[i, indices[i,j]] = shuffle[i, indices[i,j]]\n",
    "    return data_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_shap_local(X, y, fit):\n",
    "    \"\"\"\n",
    "    Compute average treeshap value across observations.\n",
    "    Larger absolute values indicate more important features.\n",
    "    :param X: design matrix\n",
    "    :param y: response\n",
    "    :param fit: fitted model of interest (tree-based)\n",
    "    :return: dataframe of shape: (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    explainer = shap.TreeExplainer(fit)\n",
    "    shap_values = explainer.shap_values(X, check_additivity=False)\n",
    "    if sklearn.base.is_classifier(fit):\n",
    "        # Shape values are returned as a list of arrays, one for each class\n",
    "        def add_abs(a, b):\n",
    "            return abs(a) + abs(b)\n",
    "        results = reduce(add_abs, shap_values)\n",
    "    else:\n",
    "        results = abs(shap_values)\n",
    "    result_table = pd.DataFrame(results, columns=[f'Feature_{i}' for i in range(X.shape[1])])\n",
    "\n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\".\")\n",
    "# sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_real_X(fpath=None, X=None, seed=None, normalize=True,\n",
    "#                   sample_row_n=None, sample_col_n=None, permute_col=True,\n",
    "#                   signal_features=None, n_signal_features=None, permute_nonsignal_col=None):\n",
    "#     \"\"\"\n",
    "#     :param fpath: path to X data\n",
    "#     :param X: data matrix\n",
    "#     :param seed: random seed\n",
    "#     :param normalize: boolean; whether or not to normalize columns in data to mean 0 and variance 1\n",
    "#     :param sample_row_n: number of samples to subset; default keeps all rows\n",
    "#     :param sample_col_n: number of features to subset; default keeps all columns\n",
    "#     :param permute_col: boolean; whether or not to permute the columns\n",
    "#     :param signal_features: list of features to use as signal features\n",
    "#     :param n_signal_features: number of signal features; required if permute_nonsignal_col is not None\n",
    "#     :param permute_nonsignal_col: how to permute the nonsignal features; must be one of\n",
    "#         [None, \"block\", \"indep\", \"augment\"], where None performs no permutation, \"block\" performs the permutation\n",
    "#         row-wise, \"indep\" permutes each nonsignal feature column independently, \"augment\" augments the signal features\n",
    "#         with the row-permuted X matrix.\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     assert permute_nonsignal_col in [None, \"block\", \"indep\", \"augment\"]\n",
    "#     if X is None:\n",
    "#         X = pd.read_csv(fpath)\n",
    "#     if normalize:\n",
    "#         X = (X - X.mean()) / X.std()\n",
    "#     if seed is not None:\n",
    "#         np.random.seed(seed)\n",
    "#     if permute_col:\n",
    "#         X = X[np.random.permutation(X.columns)]\n",
    "#     if sample_row_n is not None:\n",
    "#         keep_idx = np.random.choice(X.shape[0], sample_row_n, replace=False)\n",
    "#         X = X.iloc[keep_idx, :]\n",
    "#     if sample_col_n is not None:\n",
    "#         if signal_features is None:\n",
    "#             X = X.sample(n=sample_col_n, replace=False, axis=1)\n",
    "#         else:\n",
    "#             rand_features = np.random.choice([col for col in X.columns if col not in signal_features],\n",
    "#                                              sample_col_n - len(signal_features), replace=False)\n",
    "#             X = X[signal_features + list(rand_features)]\n",
    "#     if signal_features is not None:\n",
    "#         X = X[signal_features + [col for col in X.columns if col not in signal_features]]\n",
    "#     if permute_nonsignal_col is not None:\n",
    "#         assert n_signal_features is not None\n",
    "#         if permute_nonsignal_col == \"block\":\n",
    "#             X = np.hstack([X.iloc[:, :n_signal_features].to_numpy(),\n",
    "#                            X.iloc[np.random.permutation(X.shape[0]), n_signal_features:].to_numpy()])\n",
    "#             X = pd.DataFrame(X)\n",
    "#         elif permute_nonsignal_col == \"indep\":\n",
    "#             for j in range(n_signal_features, X.shape[1]):\n",
    "#                 X.iloc[:, j] = np.random.permutation(X.iloc[:, j])\n",
    "#         elif permute_nonsignal_col == \"augment\":\n",
    "#             X = np.hstack([X.iloc[:, :n_signal_features].to_numpy(),\n",
    "#                            X.iloc[np.random.permutation(X.shape[0]), :].to_numpy()])\n",
    "#             X = IndexedArray(pd.DataFrame(X).to_numpy(), index=keep_idx)\n",
    "#             return X\n",
    "#     return X.to_numpy()\n",
    "\n",
    "# def sample_real_Y(X, fpath=None, return_support=False):\n",
    "#     Y = pd.read_csv(fpath)\n",
    "#     if return_support:\n",
    "#         return Y.to_numpy(), np.ones(Y.shape[1]), None\n",
    "#     return Y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# X = sample_real_X('../data/X_ccle_rnaseq_cleaned_filtered5000.csv')\n",
    "# y = sample_real_Y(X, '../data/y_ccle_AZD0530.csv').flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only select the first 100 samples for demonstration\n",
    "# X = X[:,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y.shape\n",
    "(200,1)\n",
    "(200,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "d = 10\n",
    "mean = [[0]*5 + [0]*5, [10]*5 + [0]*5]\n",
    "scale = [[1]*10,[1]*10]\n",
    "s = 5\n",
    "X = sample_normal_X_subgroups(n, d, mean, scale)\n",
    "beta = np.concatenate((np.ones(s), np.zeros(d-s)))\n",
    "y = np.matmul(X, beta)\n",
    "split_seed = 0\n",
    "X_train, X_tune, X_test, y_train, y_tune, y_test = apply_splitting_strategy(X, y, \"train-test\", split_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn y.shape to (200,1)\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regressor = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, max_features=0.33, random_state=331)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Transformer representation was empty for all trees.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m# rf_plus_model = RandomForestPlusRegressor(rf_model=rf_regressor, include_raw=False)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m rf_plus_model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m----> 5\u001b[0m score \u001b[39m=\u001b[39m rf_plus_model\u001b[39m.\u001b[39;49mget_mdi_plus_scores(X_test, y_test, lfi\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, lfi_abs \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mnone\u001b[39;49m\u001b[39m\"\u001b[39;49m, sample_split\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, train_or_test \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m score[\u001b[39m\"\u001b[39m\u001b[39mlfi\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\local_MDI+\\imodels-experiments\\feature_importance\\../../imodels\\imodels\\importance\\rf_plus.py:380\u001b[0m, in \u001b[0;36m_RandomForestPlus.get_mdi_plus_scores\u001b[1;34m(self, X, y, scoring_fns, local_scoring_fns, sample_split, mode, version, lfi, lfi_abs, train_or_test)\u001b[0m\n\u001b[0;32m    368\u001b[0m mdi_plus_obj \u001b[39m=\u001b[39m ForestMDIPlus(estimators\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_,\n\u001b[0;32m    369\u001b[0m                              transformers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers_,\n\u001b[0;32m    370\u001b[0m                              scoring_fns\u001b[39m=\u001b[39mscoring_fns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m                              normalize\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize,\n\u001b[0;32m    378\u001b[0m                              version\u001b[39m=\u001b[39mversion)\n\u001b[0;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmdi_plus_ \u001b[39m=\u001b[39m mdi_plus_obj\n\u001b[1;32m--> 380\u001b[0m mdi_plus_scores \u001b[39m=\u001b[39m mdi_plus_obj\u001b[39m.\u001b[39;49mget_scores(X_array, y, lfi\u001b[39m=\u001b[39;49mlfi,\n\u001b[0;32m    381\u001b[0m                                           lfi_abs\u001b[39m=\u001b[39;49mlfi_abs,\n\u001b[0;32m    382\u001b[0m                                           train_or_test\u001b[39m=\u001b[39;49mtrain_or_test)\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m lfi \u001b[39mand\u001b[39;00m local_scoring_fns:\n\u001b[0;32m    384\u001b[0m     mdi_plus_lfi \u001b[39m=\u001b[39m mdi_plus_scores[\u001b[39m\"\u001b[39m\u001b[39mlfi\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\local_MDI+\\imodels-experiments\\feature_importance\\../../imodels\\imodels\\importance\\mdi_plus.py:127\u001b[0m, in \u001b[0;36mForestMDIPlus.get_scores\u001b[1;34m(self, X, y, lfi, lfi_abs, train_or_test)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlfi_abs \u001b[39m=\u001b[39m lfi_abs\n\u001b[0;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_or_test \u001b[39m=\u001b[39m train_or_test\n\u001b[1;32m--> 127\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_importance_scores(X, y)\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m lfi:\n\u001b[0;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_scoring_fns:\n",
      "File \u001b[1;32md:\\local_MDI+\\imodels-experiments\\feature_importance\\../../imodels\\imodels\\importance\\mdi_plus.py:237\u001b[0m, in \u001b[0;36mForestMDIPlus._fit_importance_scores\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_lfi_matrix \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(average_lfi_matrix)\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(all_scores) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 237\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTransformer representation was empty for all trees.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m full_preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmean(all_full_preds, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_full_preds \u001b[39m=\u001b[39m full_preds\n",
      "\u001b[1;31mValueError\u001b[0m: Transformer representation was empty for all trees."
     ]
    }
   ],
   "source": [
    "rf_plus_model = RandomForestPlusRegressor(rf_model=copy.deepcopy(rf_regressor), include_raw=False)\n",
    "# rf_plus_model = RandomForestPlusRegressor(rf_model=rf_regressor, include_raw=False)\n",
    "rf_plus_model.fit(X_train, y_train)\n",
    "\n",
    "score = rf_plus_model.get_mdi_plus_scores(X_test, y_test, lfi=True, lfi_abs = \"none\", sample_split=None, train_or_test = \"test\")\n",
    "score[\"lfi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_plus_model = RandomForestPlusRegressor(rf_model=copy.deepcopy(rf_regressor), include_raw=True)\n",
    "# rf_plus_model = RandomForestPlusRegressor(rf_model=rf_regressor, include_raw=True)\n",
    "rf_plus_model.fit(X_train, y_train)\n",
    "\n",
    "score = rf_plus_model.get_mdi_plus_scores(X_test, y_test, lfi=True, lfi_abs = \"none\", sample_split=None, train_or_test = \"test\")\n",
    "score[\"lfi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rf_plus_model.get_mdi_plus_scores(X_test, y_test, lfi=True, lfi_abs = \"none\", sample_split=None, train_or_test = \"test\")\n",
    "score[\"lfi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data\n",
    "n = 200\n",
    "d = 10\n",
    "mean = [[0]*5 + [0]*5, [10]*5 + [0]*5]\n",
    "scale = [[1]*10,[1]*10]\n",
    "s = 5\n",
    "X = sample_normal_X_subgroups(n, d, mean, scale)\n",
    "beta = np.concatenate((np.ones(s), np.zeros(d-s)))\n",
    "y = np.matmul(X, beta)\n",
    "split_seed = 0\n",
    "X_train, X_tune, X_test, y_train, y_tune, y_test = apply_splitting_strategy(X, y, \"train-test\", split_seed)\n",
    "\n",
    "# #Define the model and fit\n",
    "# rf_regressor = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, max_features=0.33, random_state=331)\n",
    "# rf_regressor.fit(X_train, y_train)\n",
    "# seed = 0\n",
    "# # pass in a copy of rf_regressor to avoid modifying the original model\n",
    "# rf_plus_model = RandomForestPlusRegressor(rf_model=copy.deepcopy(rf_regressor), include_raw=False)\n",
    "# rf_plus_model = RandomForestPlusRegressor(rf_model=rf_regressor, include_raw=False)\n",
    "# rf_plus_model.fit(X_train, y_train)\n",
    "\n",
    "# # initialize the metric results\n",
    "# metric_results = {}\n",
    "\n",
    "# y_pred = rf_regressor.predict(X_test)\n",
    "# metric_results['MSE_before_ablation'] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# # Ablation\n",
    "# score = rf_plus_model.get_mdi_plus_scores(X_test, y_test, lfi=True, lfi_abs = \"outside\", sample_split=None)\n",
    "# local_fi_score = score[\"lfi\"]\n",
    "# ascending = True # False for MDI\n",
    "# imp_vals = copy.deepcopy(local_fi_score)\n",
    "# imp_vals[imp_vals == float(\"-inf\")] = -sys.maxsize - 1\n",
    "# imp_vals[imp_vals == float(\"inf\")] = sys.maxsize - 1\n",
    "# seed = 0 #np.random.randint(0, 100000)\n",
    "# for i in range(X_test.shape[1]):\n",
    "#     if ascending:\n",
    "#         ablation_X_test = ablation(X_test, imp_vals, \"max\", i+1, seed)\n",
    "#     else:\n",
    "#         ablation_X_test = ablation(X_test, imp_vals, \"min\", i+1, seed)\n",
    "#     metric_results[f'MSE_after_ablation_{i+1}'] = mean_squared_error(y_test, rf_regressor.predict(ablation_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model and fit\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, max_features=0.33, random_state=331)\n",
    "rf_regressor.fit(copy.deepcopy(X_train), copy.deepcopy(y_train))\n",
    "seed = 0\n",
    "# pass in a copy of rf_regressor to avoid modifying the original model\n",
    "#rf_plus_model = RandomForestPlusRegressor(rf_model=copy.deepcopy(rf_regressor), include_raw=False)\n",
    "rf_plus_model = RandomForestPlusRegressor(rf_model=rf_regressor, include_raw=False)\n",
    "rf_plus_model.fit(copy.deepcopy(X_train), copy.deepcopy(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot metric_results\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(list(metric_results.keys()), list(metric_results.values()))\n",
    "# ax.set_xlabel('Number of features ablated')\n",
    "# ax.set_ylabel('MSE')\n",
    "# ax.set_title('MSE after ablation')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_results = {}\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "metric_results['MSE_before_ablation'] = mean_squared_error(y_test, y_pred)\n",
    "local_fi_score = tree_shap_local(X_test, y_test, rf_regressor)\n",
    "ascending = True # False for MDI\n",
    "imp_vals = copy.deepcopy(local_fi_score)\n",
    "imp_vals[imp_vals == float(\"-inf\")] = -sys.maxsize - 1\n",
    "imp_vals[imp_vals == float(\"inf\")] = sys.maxsize - 1\n",
    "seed = np.random.randint(0, 100000)\n",
    "for i in range(X_test.shape[1]):\n",
    "    if ascending:\n",
    "        ablation_X_test = ablation(X_test, imp_vals, \"max\", i+1, seed)\n",
    "    else:\n",
    "        ablation_X_test = ablation(X_test, imp_vals, \"min\", i+1, seed)\n",
    "    metric_results[f'MSE_after_ablation_{i+1}'] = mean_squared_error(y_test, rf_regressor.predict(ablation_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metric_results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(list(metric_results.keys()), list(metric_results.values()))\n",
    "ax.set_xlabel('Number of features ablated')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('MSE after ablation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 200\n",
    "# d = 10\n",
    "# mean = [[100,70,50,30,10]+[0]*5, [100,70,50,30,10]+[0]*5]\n",
    "# scale = [[1]*10,[1]*10]\n",
    "# s = 5\n",
    "# X = sample_normal_X_subgroups(n, d, mean, scale)\n",
    "# beta = np.concatenate((np.ones(s), np.zeros(d-s)))\n",
    "# y = np.matmul(X, beta)\n",
    "# split_seed = 0\n",
    "# X_train, X_tune, X_test, y_train, y_tune, y_test = apply_splitting_strategy(X, y, \"train-test\", split_seed)\n",
    "\n",
    "# #Define the model and fit\n",
    "# rf_regressor = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, max_features=0.33, random_state=331)\n",
    "# rf_regressor.fit(X_train, y_train)\n",
    "# seed = 0\n",
    "\n",
    "# metric_results = {}\n",
    "# y_pred = rf_regressor.predict(X_test)\n",
    "# metric_results['MSE_before_ablation'] = mean_squared_error(y_test, y_pred)\n",
    "# local_fi_score = tree_shap_local(X_test, y_test, rf_regressor)\n",
    "# ascending = True # False for MDI\n",
    "# imp_vals = copy.deepcopy(local_fi_score)\n",
    "# imp_vals[imp_vals == float(\"-inf\")] = -sys.maxsize - 1\n",
    "# imp_vals[imp_vals == float(\"inf\")] = sys.maxsize - 1\n",
    "# seed = np.random.randint(0, 100000)\n",
    "# for i in range(X_test.shape[1]):\n",
    "#     if ascending:\n",
    "#         ablation_X_test = ablation(X_test, imp_vals, \"max\", i+1, seed)\n",
    "#     else:\n",
    "#         ablation_X_test = ablation(X_test, imp_vals, \"min\", i+1, seed)\n",
    "#     metric_results[f'MSE_after_ablation_{i+1}'] = mean_squared_error(y_test, rf_regressor.predict(ablation_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "d = 10\n",
    "mean = [[100,70,50,30,10]+[0]*5, [100,70,50,30,10]+[0]*5]\n",
    "scale = [[1]*10,[1]*10]\n",
    "s = 5\n",
    "X = sample_normal_X_subgroups(n, d, mean, scale)\n",
    "beta = np.concatenate((np.ones(s), np.zeros(d-s)))\n",
    "y = np.matmul(X, beta)\n",
    "split_seed = 0\n",
    "X_train, X_tune, X_test, y_train, y_tune, y_test = apply_splitting_strategy(X, y, \"train-test\", split_seed)\n",
    "\n",
    "#Define the model and fit\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, max_features=0.33, random_state=331)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "seed = 0\n",
    "\n",
    "metric_results = {}\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "metric_results['MSE_before_ablation'] = mean_squared_error(y_test, y_pred)\n",
    "local_fi_score = tree_shap_local(X_test, y_test, rf_regressor)\n",
    "ascending = True # False for MDI\n",
    "imp_vals = copy.deepcopy(local_fi_score)\n",
    "imp_vals[imp_vals == float(\"-inf\")] = -sys.maxsize - 1\n",
    "imp_vals[imp_vals == float(\"inf\")] = sys.maxsize - 1\n",
    "seed = np.random.randint(0, 100000)\n",
    "for i in range(X_test.shape[1]):\n",
    "    if ascending:\n",
    "        ablation_X_test = ablation(X_test, imp_vals, \"max\", i+1, seed)\n",
    "    else:\n",
    "        ablation_X_test = ablation(X_test, imp_vals, \"min\", i+1, seed)\n",
    "    metric_results[f'MSE_after_ablation_{i+1}'] = mean_squared_error(y_test, rf_regressor.predict(ablation_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metric_results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(list(metric_results.keys()), list(metric_results.values()))\n",
    "ax.set_xlabel('Number of features ablated')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('MSE after ablation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "d = 10\n",
    "mean = [[100,70,50,30,10]+[0]*5, [10,7,5,3,1]+[0]*5]\n",
    "scale = [[1]*10,[1]*10]\n",
    "s = 5\n",
    "X = sample_normal_X_subgroups(n, d, mean, scale)\n",
    "beta = np.concatenate((np.ones(s), np.zeros(d-s)))\n",
    "y = np.matmul(X, beta)\n",
    "split_seed = 0\n",
    "X_train, X_tune, X_test, y_train, y_tune, y_test = apply_splitting_strategy(X, y, \"train-test\", split_seed)\n",
    "\n",
    "#Define the model and fit\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, max_features=0.33, random_state=331)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "seed = 0\n",
    "\n",
    "metric_results = {}\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "metric_results['MSE_before_ablation'] = mean_squared_error(y_test, y_pred)\n",
    "local_fi_score = tree_shap_local(X_test, y_test, rf_regressor)\n",
    "ascending = True # False for MDI\n",
    "imp_vals = copy.deepcopy(local_fi_score)\n",
    "imp_vals[imp_vals == float(\"-inf\")] = -sys.maxsize - 1\n",
    "imp_vals[imp_vals == float(\"inf\")] = sys.maxsize - 1\n",
    "seed = np.random.randint(0, 100000)\n",
    "for i in range(X_test.shape[1]):\n",
    "    if ascending:\n",
    "        ablation_X_test = ablation(X_test, imp_vals, \"max\", i+1, seed)\n",
    "    else:\n",
    "        ablation_X_test = ablation(X_test, imp_vals, \"min\", i+1, seed)\n",
    "    metric_results[f'MSE_after_ablation_{i+1}'] = mean_squared_error(y_test, rf_regressor.predict(ablation_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metric_results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(list(metric_results.keys()), list(metric_results.values()))\n",
    "ax.set_xlabel('Number of features ablated')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('MSE after ablation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coolwarm_camp = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "shap_val = tree_shap_local(X_test, y_test, rf_regressor)\n",
    "shap_val.columns = [f'{i}' for i in range(X.shape[1])]\n",
    "sns.heatmap(shap_val, cmap=coolwarm_camp, xticklabels=\"auto\", yticklabels=\"auto\")\n",
    "plt.title(\"SHAP Values\")\n",
    "plt.xlabel(\"Feature Number\")\n",
    "plt.ylabel(\"Feature Importance Score - higher absolute better\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
